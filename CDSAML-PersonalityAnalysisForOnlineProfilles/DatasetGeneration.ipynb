{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DatasetGeneration.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNM40Px5P6T8y0nHzntpqz6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ghanashyam-Bhat/Summer_2/blob/main/CDSAML-PersonalityAnalysisForOnlineProfilles/DatasetGeneration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 274,
      "metadata": {
        "id": "s1viYmMRVtRi"
      },
      "outputs": [],
      "source": [
        "#pip install fer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#pip install image-quality"
      ],
      "metadata": {
        "id": "RGdbeSq3V0Jb"
      },
      "execution_count": 275,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Sl0HlQhkV2t9"
      },
      "execution_count": 276,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import imquality.brisque as brisque\n",
        "import cv2 as cv\n",
        "import numpy as np\n",
        "import time\n",
        "from fer import *\n",
        "import matplotlib.pyplot as plt \n",
        "from keras.preprocessing import image\n",
        "from google.colab.patches import cv2_imshow\n",
        "%matplotlib inline\n",
        "import urllib\n",
        "import requests\n",
        "\n",
        "from scipy.spatial import distance as dist\n",
        "from imutils.video import FileVideoStream\n",
        "from imutils.video import VideoStream\n",
        "from imutils import face_utils\n",
        "# import numpy as np\n",
        "import argparse\n",
        "import imutils\n",
        "# import time\n",
        "import dlib\n",
        "# import cv2"
      ],
      "metadata": {
        "id": "1rOrH3L7V4Nr"
      },
      "execution_count": 277,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def resizing(base,path):\n",
        "  image_file = Image.open(path)\n",
        "  image_file.save(base+\"edited.jpg\", quality=30)"
      ],
      "metadata": {
        "id": "65mSQyAtWe85"
      },
      "execution_count": 278,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def quality_assesment(base):\n",
        "  img = Image.open(base+\"edited.jpg\")\n",
        "  count_quality = brisque.score(img)\n",
        "  return count_quality\n"
      ],
      "metadata": {
        "id": "VPUF1KSYWtGi"
      },
      "execution_count": 279,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def face_detection(path2):\n",
        "  #https://github.com/rk45825243/Face-eye-detection-using-Haar-Cascade-classifier\n",
        "  img = cv.imread(path2)\n",
        "\n",
        "  time.sleep(0.2)\n",
        "  gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
        "\n",
        "  face_cascade = cv.CascadeClassifier('/content/drive/MyDrive/cdsaml/haarcasscade/haarcascade_frontalface_alt2.xml')\n",
        "  eye_cascade = cv.CascadeClassifier(r'/content/drive/MyDrive/cdsaml/haarcasscade/haarcascade_eye.xml')\n",
        "  lefteye_cascade = cv.CascadeClassifier('/content/drive/MyDrive/cdsaml/haarcasscade/haarcascade_lefteye_2splits.xml')\n",
        "  righteye_cascade = cv.CascadeClassifier('/content/drive/MyDrive/cdsaml/haarcasscade/haarcascade_righteye_2splits.xml')\n",
        "  faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
        "\n",
        "  count_eyes = 0\n",
        "  eyes_open = 1\n",
        "\n",
        "  count_face = 0\n",
        "  face_areas = []\n",
        "  ph,pw,pc = img.shape\n",
        "  pic_area = ph*pw\n",
        "  eye_area = []\n",
        "  pic_mid_point = (pw/2,ph/2)\n",
        "  face_mid_points = []\n",
        "  #print(ph,pw)\n",
        "  for (x, y, w, h) in faces:\n",
        "    cv.rectangle(img, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
        "    roi_gray = gray[y:y + h, x:x + w]\n",
        "    roi_color = img[y:y + h, x:x + w]\n",
        "    face_areas.append(w*h)\n",
        "    face_mid_points.append((x+w/2,y+h/2))\n",
        "    count_face +=1 \n",
        "    #print(h,w)\n",
        "\n",
        "    eyes = eye_cascade.detectMultiScale(roi_gray)\n",
        "    for (ex, ey, ew, eh) in eyes:\n",
        "        cv.rectangle(roi_color, (ex, ey), (ex + ew, ey + eh), (0, 255, 0), 2)\n",
        "        count_eyes+=1 \n",
        "\n",
        "  eye = 0\n",
        "  openEye = 0\n",
        "  counter = 0\n",
        "  openEyes = eye_cascade.detectMultiScale(roi_gray)\n",
        "  AllEyes = lefteye_cascade.detectMultiScale(roi_gray)\n",
        "  AllEyes2 = righteye_cascade.detectMultiScale(roi_gray)\n",
        "  for (ex, ey, ew, eh) in openEyes:\n",
        "    cv.rectangle(roi_color, (ex, ey), (ex + ew, ey + eh), (0, 255, 0),2)\n",
        "    openEye += 1\n",
        "\n",
        "  for (ex, ey, ew, eh) in AllEyes:\n",
        "    eye += 1\n",
        "    cv.rectangle(roi_color, (ex, ey), (ex + ew, ey + eh), (0, 0, 40),2)\n",
        "    eye_area.append(ew*eh)\n",
        "\n",
        "  for (ex, ey, ew, eh) in AllEyes2:\n",
        "    counter += 1\n",
        "    cv.rectangle(roi_color, (ex, ey), (ex + ew, ey + eh), (0, 0, 40),2)\n",
        "    eye_area.append(ew*eh)\n",
        "\n",
        "  if (openEye != eye):\n",
        "    print ('Eyes Closed')\n",
        "    eyes_open = 0\n",
        "\n",
        "\n",
        "  print(eye,counter,openEye)\n",
        "\n",
        "  eye_area.sort(reverse=True)\n",
        "  cv2_imshow( img)\n",
        "  cv.destroyAllWindows()\n",
        "  return count_face,count_eyes,face_areas,pic_area,face_mid_points,pic_mid_point,eye_area"
      ],
      "metadata": {
        "id": "DsK3RxqlXHMv"
      },
      "execution_count": 280,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def background_detection(path2):\n",
        "  img = cv.imread(path2)\n",
        "  #cv2_imshow(img)\n",
        "  #cv.waitKey(1)\n",
        "\n",
        "  # Load names of classes and get random colors\n",
        "  classes = open('/content/drive/MyDrive/cdsaml/darknet/data/coco.names').read().strip().split('\\n')\n",
        "  np.random.seed(42)\n",
        "  colors = np.random.randint(0, 255, size=(len(classes), 3), dtype='uint8')\n",
        "\n",
        "  # Give the configuration and weight files for the model and load the network.\n",
        "  net = cv.dnn.readNetFromDarknet('/content/drive/MyDrive/cdsaml/darknet/cfg/yolov3.cfg', '/content/drive/MyDrive/cdsaml/darknet/yolov3.weights')\n",
        "  net.setPreferableBackend(cv.dnn.DNN_BACKEND_OPENCV)\n",
        "  # net.setPreferableTarget(cv.dnn.DNN_TARGET_CPU)\n",
        "\n",
        "  # determine the output layer\n",
        "  ln = net.getLayerNames()\n",
        "  #print(net.getUnconnectedOutLayers())\n",
        "  ln = [ln[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
        "\n",
        "  # construct a blob from the image\n",
        "  blob = cv.dnn.blobFromImage(img, 1/255.0, (416, 416), swapRB=True, crop=False)\n",
        "  r = blob[0, 0, :, :]\n",
        "\n",
        "  #cv2_imshow( r)\n",
        "  text = f'Blob shape={blob.shape}'\n",
        "  #cv.displayOverlay('blob', text)\n",
        "  #cv.waitKey(1)\n",
        "\n",
        "  net.setInput(blob)\n",
        "  t0 = time.time()\n",
        "  outputs = net.forward(ln)\n",
        "  t = time.time()\n",
        "  #print('time=', t-t0)\n",
        "\n",
        "  #print(len(outputs))\n",
        "  #for out in outputs:\n",
        "      #print(out.shape)\n",
        "\n",
        "  # def trackbar2(x):\n",
        "  #     confidence = x/100\n",
        "  #     r = r0.copy()\n",
        "  #     for output in np.vstack(outputs):\n",
        "  #         if output[4] > confidence:\n",
        "  #             x, y, w, h = output[:4]\n",
        "  #             p0 = int((x-w/2)*416), int((y-h/2)*416)\n",
        "  #             p1 = int((x+w/2)*416), int((y+h/2)*416)\n",
        "  #             cv.rectangle(r, p0, p1, 1, 1)\n",
        "  #     #cv.imshow(r)\n",
        "  #     text = f'Bbox confidence={confidence}'\n",
        "  #     #cv.displayOverlay('blob', text)\n",
        "\n",
        "  r0 = blob[0, 0, :, :]\n",
        "  r = r0.copy()\n",
        "  #cv2_imshow( r)\n",
        "  #cv.createTrackbar('confidence', 'blob', 50, 101, trackbar2)\n",
        "  #trackbar2(50)\n",
        "\n",
        "  boxes = []\n",
        "  confidences = []\n",
        "  classIDs = []\n",
        "  h, w = img.shape[:2]\n",
        "\n",
        "  for output in outputs:\n",
        "      for detection in output:\n",
        "          scores = detection[5:]\n",
        "          classID = np.argmax(scores)\n",
        "          confidence = scores[classID]\n",
        "          if confidence > 0.5:\n",
        "              box = detection[:4] * np.array([w, h, w, h])\n",
        "              (centerX, centerY, width, height) = box.astype(\"int\")\n",
        "              x = int(centerX - (width / 2))\n",
        "              y = int(centerY - (height / 2))\n",
        "              box = [x, y, int(width), int(height)]\n",
        "              boxes.append(box)\n",
        "              confidences.append(float(confidence))\n",
        "              classIDs.append(classID)\n",
        "\n",
        "  indices = cv.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
        "  count_objects = 0\n",
        "  count_person = 0\n",
        "  if len(indices) > 0:\n",
        "      for i in indices.flatten():\n",
        "          (x, y) = (boxes[i][0], boxes[i][1])\n",
        "          (w, h) = (boxes[i][2], boxes[i][3])\n",
        "          color = [int(c) for c in colors[classIDs[i]]]\n",
        "          cv.rectangle(img, (x, y), (x + w, y + h), color, 2)\n",
        "          text = \"{}: {:.4f}\".format(classes[classIDs[i]], confidences[i])\n",
        "          cv.putText(img, text, (x, y - 5), cv.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)\n",
        "          if \"person\" not in text:\n",
        "              count_objects += 1\n",
        "          else:\n",
        "              count_person += 1\n",
        "\n",
        "  cv2_imshow(img)\n",
        "  cv.waitKey(0)\n",
        "  cv.destroyAllWindows()\n",
        "  return count_person,count_objects\n"
      ],
      "metadata": {
        "id": "DNr5RBQ6Ziaj"
      },
      "execution_count": 281,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def emotion_detection(path2):\n",
        "  #https://towardsdatascience.com/the-ultimate-guide-to-emotion-recognition-from-facial-expressions-using-python-64e58d4324ff\n",
        "  test_image_one = plt.imread(path2)\n",
        "  emo_detector = FER(mtcnn=True)\n",
        "\n",
        "  # Capture all the emotions on the image\n",
        "  captured_emotions = emo_detector.detect_emotions(test_image_one)\n",
        "\n",
        "  # Print all captured emotions with the image\n",
        "  print(captured_emotions)\n",
        "  plt.imshow(test_image_one)\n",
        "\n",
        "  # Use the top Emotion() function to call for the dominant emotion in the image\n",
        "  dominant_emotion, emotion_score = emo_detector.top_emotion(test_image_one)\n",
        "  return dominant_emotion,emotion_score,captured_emotions\n"
      ],
      "metadata": {
        "id": "ohYIr1-1Z3bX"
      },
      "execution_count": 282,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eye_aspect_ratio(eye):\n",
        "    # compute the euclidean distances between the two sets of\n",
        "    # vertical eye landmarks (x, y)-coordinates\n",
        "    A = dist.euclidean(eye[1], eye[5])\n",
        "    B = dist.euclidean(eye[2], eye[4])\n",
        "\n",
        "    # compute the euclidean distance between the horizontal\n",
        "    # eye landmark (x, y)-coordinates\n",
        "    C = dist.euclidean(eye[0], eye[3])\n",
        "\n",
        "    # compute the eye aspect ratio\n",
        "    ear = (A + B) / (2.0 * C)\n",
        "\n",
        "    # return the eye aspect ratio\n",
        "    return ear\n"
      ],
      "metadata": {
        "id": "3eUPmKFfaGE1"
      },
      "execution_count": 283,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def open_closed_eye(path2):\n",
        "  frame = cv.imread(path2)\n",
        "  is_open = 0\n",
        "  # import the necessary packages\n",
        "  # frames the eye must be below the threshold\n",
        "  EYE_AR_THRESH = 0.35\n",
        "  EYE_AR_CONSEC_FRAMES = 3\n",
        "\n",
        "  # initialize the frame counters and the total number of blinks\n",
        "  COUNTER = 0\n",
        "  TOTAL = 0\n",
        "\n",
        "  # initialize dlib's face detector (HOG-based) and then create\n",
        "  # the facial landmark predictor\n",
        "  print(\"[INFO] loading facial landmark predictor...\")\n",
        "  detector = dlib.get_frontal_face_detector()\n",
        "  predictor = dlib.shape_predictor(\"/content/drive/MyDrive/cdsaml/Dlib/shape_predictor_68_face_landmarks.dat\")\n",
        "\n",
        "  # grab the indexes of the facial landmarks for the left and\n",
        "  # right eye, respectively\n",
        "  (lStart, lEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"left_eye\"]\n",
        "  (rStart, rEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"right_eye\"]\n",
        "\n",
        "  # vs = VideoStream(src=0).start()\n",
        "  # # vs = VideoStream(usePiCamera=True).start()\n",
        "  # time.sleep(1.0)\n",
        "\n",
        "  # loop over frames from the video stream\n",
        "  #frame = imutils.resize(frame, width=450)\n",
        "  gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n",
        "\n",
        "  # detect faces in the grayscale frame\n",
        "  rects = detector(gray, 0)\n",
        "  print(rects)\n",
        "  # loop over the face detections\n",
        "  for rect in rects:\n",
        "      # determine the facial landmarks for the face region, then\n",
        "      # convert the facial landmark (x, y)-coordinates to a NumPy\n",
        "      # array\n",
        "      shape = predictor(gray, rect)\n",
        "      shape = face_utils.shape_to_np(shape)\n",
        "\n",
        "      # extract the left and right eye coordinates, then use the\n",
        "      # coordinates to compute the eye aspect ratio for both eyes\n",
        "      leftEye = shape[lStart:lEnd]\n",
        "      rightEye = shape[rStart:rEnd]\n",
        "      leftEAR = eye_aspect_ratio(leftEye)\n",
        "      rightEAR = eye_aspect_ratio(rightEye)\n",
        "\n",
        "      # average the eye aspect ratio together for both eyes\n",
        "      ear = (leftEAR + rightEAR)\n",
        "\n",
        "      # compute the convex hull for the left and right eye, then\n",
        "      # visualize each of the eyes\n",
        "      leftEyeHull = cv.convexHull(leftEye)\n",
        "      rightEyeHull = cv.convexHull(rightEye)\n",
        "      cv.drawContours(frame, [leftEyeHull], -1, (0, 255, 0), 1)\n",
        "      cv.drawContours(frame, [rightEyeHull], -1, (0, 255, 0), 1)\n",
        "\n",
        "      # check to see if the eye aspect ratio is below the blink\n",
        "      # threshold, and if so, increment the blink frame counter\n",
        "      if ear < EYE_AR_THRESH:\n",
        "          cv.putText(frame, \"Eye: {}\".format(\"close\"), (10, 30),\n",
        "                  cv.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
        "          is_open = 0\n",
        "          print(\"Closed Eyes\")\n",
        "          cv.putText(frame, \"EAR: {:.2f}\".format(ear), (300, 30),\n",
        "                  cv.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
        "\n",
        "\n",
        "      # otherwise, the eye aspect ratio is not below the blink\n",
        "      # threshold\n",
        "      else:\n",
        "          cv.putText(frame, \"Eye: {}\".format(\"Open\"), (10, 30),\n",
        "                  cv.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
        "          is_open = 1\n",
        "          print(\"Open Eyes\")\n",
        "          cv.putText(frame, \"EAR: {:.2f}\".format(ear), (300, 30),\n",
        "                  cv.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
        "\n",
        "  # draw the total number of blinks on the frame along with\n",
        "  # the computed eye aspect ratio for the frame\n",
        "\n",
        "  # show the frame\n",
        "\n",
        "\n",
        "  # do a bit of cleanup\n",
        "  cv.destroyAllWindows()\n",
        "  # vs.stop()\n",
        "  return is_open"
      ],
      "metadata": {
        "id": "CxogbqjfbJ97"
      },
      "execution_count": 284,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base = \"/content/drive/MyDrive/cdsaml/image/\"\n",
        "base2 = \"/content/drive/MyDrive/cdsaml/in-the-wild-images\"\n",
        "path = \"/content/drive/MyDrive/cdsaml/image/4.jpg\"\n",
        "path2 = base+\"edited.jpg\""
      ],
      "metadata": {
        "id": "yFy-musKr8pd"
      },
      "execution_count": 285,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for i in range(70):\n",
        "#   folder = \"0\"*(2-len(str(i)))+folder+\"000\"\n",
        "#   for j in range(1000):\n",
        "#     file = folder+\"0\"*(3-len(str(j)))+str(j)\n",
        "#     path = base+\"/\"+folder+\"/\"+file+\"png\""
      ],
      "metadata": {
        "id": "6aWStH87voqT"
      },
      "execution_count": 286,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# resizing(base,path)\n",
        "# count_quality = quality_assesment(base)"
      ],
      "metadata": {
        "id": "Q2HFFrtBbKjp"
      },
      "execution_count": 287,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# count_face,count_eyes,face_areas,pic_area,face_mid_points,pic_mid_point,eye_area = face_detection(path2)\n",
        "# count_person,count_objects = background_detection(path2)\n"
      ],
      "metadata": {
        "id": "6Z6aZ0cBemP1"
      },
      "execution_count": 288,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dominant_emotion,emotion_score,captured_emotions = emotion_detection(path2)"
      ],
      "metadata": {
        "id": "yM-QWYV_eu8f"
      },
      "execution_count": 289,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eyes_open = open_closed_eye(path2)"
      ],
      "metadata": {
        "id": "pWkYcmPRezD8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3510977-9e2f-4eb0-f5e5-061c6a28add8"
      },
      "execution_count": 290,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] loading facial landmark predictor...\n",
            "rectangles[[(667, 410) (1438, 1181)]]\n",
            "Open Eyes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print(\"Total number of objects: \",count_objects)\n",
        "# print(\"Total number of people: \",count_person)\n",
        "# print(\"Total number of face: \",count_face)\n",
        "# print(\"Total number of eyes: \",count_eyes)\n",
        "# print(\"Image Quality: \",count_quality)\n",
        "# if count_face>=1:\n",
        "#   print(\"Face area ratio: \",max(face_areas)/pic_area)\n",
        "#   print(\"Face mid point: \",face_mid_points[face_areas.index(max(face_areas))])\n",
        "# print(\"Picture mid point: \",pic_mid_point)\n",
        "# if len(eye_area)>=2:\n",
        "#   print(\"Eye ratio: \",eye_area[1]/eye_area[0])\n",
        "# print(\"Emortion: \",dominant_emotion,\"(\",emotion_score,\")\")\n",
        "if eyes_open:\n",
        "  print(\"Open Eyes\")\n",
        "else:\n",
        "  print(\"Close Eyes\")"
      ],
      "metadata": {
        "id": "qo0e7y7kfO1i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2ac8391-480e-4d05-9e74-2421532faa60"
      },
      "execution_count": 291,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Open Eyes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import csv\n",
        "# with open(base+\"dataset.csv\",\"w\") as fs:\n",
        "#   write = csv.writer(fs,delimiter=\",\")\n",
        "#   write.writerow([\"Count\",\"Quality\",\"Faces\",\"Eyes\"])\n",
        "#   write.writerow([5,count_quality,count_face,count_eyes])"
      ],
      "metadata": {
        "id": "BCLqhjUxjN3d"
      },
      "execution_count": 292,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "UHT7Oe7vqsyU"
      },
      "execution_count": 292,
      "outputs": []
    }
  ]
}